{"version":3,"sources":["webpack://@causalNet/optimizers/webpack/universalModuleDefinition","webpack://@causalNet/optimizers/webpack/bootstrap","webpack://@causalNet/optimizers//home/huynhnguyen/github/causality/node_modules/@babel/runtime/helpers/interopRequireDefault.js","webpack://@causalNet/optimizers/external \"causal-net.core\"","webpack://@causalNet/optimizers/./src/StochasticGradientDescent/SGDFitParams.mixins.js","webpack://@causalNet/optimizers/./src/StochasticGradientDescent/causalNetSGDOptimizer.js","webpack://@causalNet/optimizers/./src/StochasticGradientDescent/adamOptimizer.js","webpack://@causalNet/optimizers/external \"causal-net.utils\"","webpack://@causalNet/optimizers/./src/StochasticGradientDescent/functor.js","webpack://@causalNet/optimizers/./src/trainer.mixins.js","webpack://@causalNet/optimizers/./src/evaluator.mixins.js"],"names":["root","factory","exports","module","require","define","amd","this","__WEBPACK_EXTERNAL_MODULE__1__","__WEBPACK_EXTERNAL_MODULE__7__","installedModules","__webpack_require__","moduleId","i","l","modules","call","m","c","d","name","getter","o","Object","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","prototype","hasOwnProperty","p","s","obj","default","BaseOptimizerClass","fit","lossFn","trainableVars","trainer","minimize","constructor","adam","optimizerParams","AdamOptimizer","platform","mixWith","BaseTensor","SGDFitParamsMixins","super","learningRate","beta1","beta2","epsilon","functor","mergeParams","params","T","train","Params","BaseFunctor","defaultOptimizerParameters","R","mergeDeepLeft","BasePipelineClass","Trainer","Loss","LossModel","Optimizer","sampleTensor","labelTensor","tidy","optimizer","Error","TrainDataGenerator","trainDataGenerator","[object Object]","numEpochs","batchSize","F","CoreFunctor","losses","logger","Logger","Promise","async","resolve","reject","progressBegin","epochIdx","range","trainData","iterLosses","_ref","samples","labels","tensor","asType","loss","push","data","mean","progressUpdate","progressEnd","setByConfig","pipelineConfig","groupBegin","Net","LayerRunner","Dataset","groupEnd","TestDataGenerator","testDataGenerator","OneHotPredictModel","testData","pass","predictLabelTensor","correctPredicts","mul","sum","accuracy"],"mappings":"CAAA,SAAAA,EAAAC,GACA,iBAAAC,SAAA,iBAAAC,OACAA,OAAAD,QAAAD,EAAAG,QAAA,mBAAAA,QAAA,qBACA,mBAAAC,eAAAC,IACAD,OAAA,uCAAAJ,GACA,iBAAAC,QACAA,QAAA,yBAAAD,EAAAG,QAAA,mBAAAA,QAAA,qBAEAJ,EAAA,yBAAAC,EAAAD,EAAA,mBAAAA,EAAA,qBARA,CASCO,KAAA,SAAAC,EAAAC,GACD,mBCTA,IAAAC,EAAA,GAGA,SAAAC,EAAAC,GAGA,GAAAF,EAAAE,GACA,OAAAF,EAAAE,GAAAV,QAGA,IAAAC,EAAAO,EAAAE,GAAA,CACAC,EAAAD,EACAE,GAAA,EACAZ,QAAA,IAUA,OANAa,EAAAH,GAAAI,KAAAb,EAAAD,QAAAC,IAAAD,QAAAS,GAGAR,EAAAW,GAAA,EAGAX,EAAAD,QA0DA,OArDAS,EAAAM,EAAAF,EAGAJ,EAAAO,EAAAR,EAGAC,EAAAQ,EAAA,SAAAjB,EAAAkB,EAAAC,GACAV,EAAAW,EAAApB,EAAAkB,IACAG,OAAAC,eAAAtB,EAAAkB,EAAA,CAA0CK,YAAA,EAAAC,IAAAL,KAK1CV,EAAAgB,EAAA,SAAAzB,GACA,oBAAA0B,eAAAC,aACAN,OAAAC,eAAAtB,EAAA0B,OAAAC,YAAA,CAAwDC,MAAA,WAExDP,OAAAC,eAAAtB,EAAA,cAAiD4B,OAAA,KAQjDnB,EAAAoB,EAAA,SAAAD,EAAAE,GAEA,GADA,EAAAA,IAAAF,EAAAnB,EAAAmB,IACA,EAAAE,EAAA,OAAAF,EACA,KAAAE,GAAA,iBAAAF,QAAAG,WAAA,OAAAH,EACA,IAAAI,EAAAX,OAAAY,OAAA,MAGA,GAFAxB,EAAAgB,EAAAO,GACAX,OAAAC,eAAAU,EAAA,WAAyCT,YAAA,EAAAK,UACzC,EAAAE,GAAA,iBAAAF,EAAA,QAAAM,KAAAN,EAAAnB,EAAAQ,EAAAe,EAAAE,EAAA,SAAAA,GAAgH,OAAAN,EAAAM,IAAqBC,KAAA,KAAAD,IACrI,OAAAF,GAIAvB,EAAA2B,EAAA,SAAAnC,GACA,IAAAkB,EAAAlB,KAAA8B,WACA,WAA2B,OAAA9B,EAAA,SAC3B,WAAiC,OAAAA,GAEjC,OADAQ,EAAAQ,EAAAE,EAAA,IAAAA,GACAA,GAIAV,EAAAW,EAAA,SAAAiB,EAAAC,GAAsD,OAAAjB,OAAAkB,UAAAC,eAAA1B,KAAAuB,EAAAC,IAGtD7B,EAAAgC,EAAA,GAIAhC,IAAAiC,EAAA,mBC5EAzC,EAAAD,QANA,SAAA2C,GACA,OAAAA,KAAAZ,WAAAY,EAAA,CACAC,QAAAD,mBCFA1C,EAAAD,QAAAM,qKCA4BuC,IAAuB,cAAcA,EAC7DC,IAAIC,EAAQC,GACR,OAAO3C,KAAK4C,QAAQC,SAASH,GAAQ,EAAMC,soCCYpC,IARf,MACIG,eAEAC,KAAKC,GACD,OAAO,IAAIC,UAAcD,8QCNlB,cAA4BE,WAASC,QAAQC,SACxD,CAAEC,aACFP,YAAYE,EAAgB,IACxBM,QAEA,MAAMC,aAACA,EAADC,MAAeA,EAAfC,MAAsBA,EAAtBC,QAA6BA,GAAWC,UAAQC,YAAYZ,EAD1C,CAACQ,MAAO,GAAKC,MAAO,GAAKC,QAAQ,MAEzD1D,KAAK6D,OAAS,CAAEN,eAAcC,QAAOC,QAAOC,WAC5C1D,KAAK4C,QAAU5C,KAAK8D,EAAEC,MAAMhB,KAAKQ,EAAcC,EAAOC,EAAOC,GAGjEM,aACI,OAAOhE,KAAK6D,yDCfpBjE,EAAAD,QAAAO,4KCYe,IAXf,cAAsB+D,UAClBnB,cACIQ,QAGJM,YAAYZ,EAAiBkB,GACzB,MAAMC,EAAInE,KAAKmE,EACf,OAAOA,EAAEC,cAAcpB,EAAiBkB,qNCRzBG,IAAqB,cAAcA,EAEtDC,cACI,MAAMR,EAAI9D,KAAK8D,EACTS,EAAKvE,KAAKwE,UAAWC,EAAUzE,KAAKyE,UAC1C,MAAO,CAACC,EAAcC,IAIXF,EAAUhC,IAHF,IACJqB,EAAEc,KAAM,IAAaL,EAAKG,EAAcC,KAM3DF,gBACI,IAAIzE,KAAK6E,UACL,MAAMC,MAAM,wBAEhB,OAAO9E,KAAK6E,UAIhBJ,cAAcI,GACV7E,KAAK6E,UAAYA,EAGrBE,yBACI,IAAI/E,KAAKgF,mBACL,MAAMF,MAAM,iCAEhB,OAAO9E,KAAKgF,mBAEhBD,uBAAuBA,GACnB/E,KAAKgF,mBAAqBD,EAG9BE,YAAYC,EAAWC,GAEnB,MAAMC,EAAIpF,KAAKoF,EAAGjB,EAAInE,KAAKoF,EAAEC,YAAavB,EAAI9D,KAAK8D,EAC7CiB,EAAqB/E,KAAK+E,mBAAoBT,EAAUtE,KAAKsE,QACnE,IAAIgB,EAAS,GAAIC,EAASvF,KAAKwF,OAC/B,OAAO,IAAIC,QAAQC,MAAOC,EAASC,KAC/BL,EAAOM,cAAcX,GACrB,IAAI,IAAIY,KAAYV,EAAEW,MAAMb,GAAW,CACnC,MAAMc,EAAYjB,EAAmBI,GACrC,IAAIc,EAAa,GACjB,cAAAC,KAAsCF,EAAU,KAAjCG,QAAEA,EAAFC,OAAWA,GAAsBF,EACxCxB,EAAeZ,EAAEuC,OAAOF,GAASG,OAAO,WACxC3B,EAAcb,EAAEuC,OAAOD,GAAQE,OAAO,WACtCC,EAAOjC,EAAQI,EAAcC,GACjCsB,EAAWO,WAAWD,EAAKE,QAE/BnB,EAAOkB,KAAKrC,EAAEuC,KAAKT,IACnBA,EAAa,GACbV,EAAOoB,eAAe,CAACb,WAAUR,SAAQJ,cAE7CK,EAAOqB,cACPjB,EAAQ,CAACL,aAKjBuB,YAAYC,GACLxD,MAAMuD,aACLvD,MAAMuD,YAAYC,GAEtB9G,KAAKwF,OAAOuB,WAAW,yBACvB,MAAMtC,UAAEA,GAAcqC,EAAeE,IACrChH,KAAKyE,UAAYA,EACjBA,EAAUwC,YAAcjH,KAAKiH,YAC7BjH,KAAK+E,mBAAqB+B,EAAeI,QAAQnC,mBACjD/E,KAAKwF,OAAO2B,8NCtEK9C,IAAqB,cAAcA,EAExD+C,wBACI,IAAIpH,KAAKqH,kBACL,MAAMvC,MAAM,gCAEhB,OAAO9E,KAAKqH,kBAEhBD,sBAAsBC,GAClBrH,KAAKqH,kBAAoBA,EAG7BpC,WAAWE,EAAU,IACjB,MAAMrB,EAAI9D,KAAK8D,EAAGK,EAAInE,KAAKoF,EAAEC,YACvB+B,EAAoBpH,KAAKoH,kBAAmBE,EAAqBtH,KAAKsH,mBAC5E,OAAO,IAAI7B,QAAQC,MAAOC,EAASC,KAC/B,MAAM2B,EAAWH,EAAkBjC,GACnC,IAAIqC,EAAO,GACX,cAAAtB,KAAsCqB,EAAS,KAAhCpB,QAAEA,EAAFC,OAAWA,GAAqBF,EACvCxB,EAAeZ,EAAEuC,OAAOF,GAASG,OAAO,WACxC3B,EAAcb,EAAEuC,OAAOD,GAAQE,OAAO,WACtCmB,EAAqBH,EAAmB5C,GACxCgD,EAAkBD,EAAmBE,IAAIhD,GAC7C6C,EAAO,IAAIA,WAAgBE,EAAgBE,IAAI,GAAGnB,QAEtD,IAAIoB,EAAW1D,EAAEuC,KAAKc,GACtB7B,EAAQ,CAACkC,WAAUL,WAK3BX,YAAYC,GACLxD,MAAMuD,aACLvD,MAAMuD,YAAYC,GAEtB9G,KAAKwF,OAAOuB,WAAW,2BACvB/G,KAAKoH,kBAAoBN,EAAeI,QAAQE,kBAChDpH,KAAKwF,OAAO2B","file":"@causalNet/optimizers.web.js","sourcesContent":["(function webpackUniversalModuleDefinition(root, factory) {\n\tif(typeof exports === 'object' && typeof module === 'object')\n\t\tmodule.exports = factory(require(\"causal-net.core\"), require(\"causal-net.utils\"));\n\telse if(typeof define === 'function' && define.amd)\n\t\tdefine([\"causal-net.core\", \"causal-net.utils\"], factory);\n\telse if(typeof exports === 'object')\n\t\texports[\"@causalNet/optimizers\"] = factory(require(\"causal-net.core\"), require(\"causal-net.utils\"));\n\telse\n\t\troot[\"@causalNet/optimizers\"] = factory(root[\"causal-net.core\"], root[\"causal-net.utils\"]);\n})(this, function(__WEBPACK_EXTERNAL_MODULE__1__, __WEBPACK_EXTERNAL_MODULE__7__) {\nreturn "," \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 3);\n","function _interopRequireDefault(obj) {\n  return obj && obj.__esModule ? obj : {\n    \"default\": obj\n  };\n}\n\nmodule.exports = _interopRequireDefault;","module.exports = __WEBPACK_EXTERNAL_MODULE__1__;","const SGDFitParamsMixins = (BaseOptimizerClass) => class extends BaseOptimizerClass{\n    fit(lossFn, trainableVars){\n        return this.trainer.minimize(lossFn, true, trainableVars);\n    }\n};\n\nexport default SGDFitParamsMixins;","import {default as AdamOptimizer} from './adamOptimizer';\n/**\n * This CausalNetSGDOptimizer provide SGD optimizers based on Tensorflowjs optimizer\n *\n * @class CausalNetSGDOptimizer\n */\nclass CausalNetSGDOptimizer{\n    constructor(){\n    }\n    adam(optimizerParams){\n        return new AdamOptimizer(optimizerParams); \n    }\n}\n\nexport default new CausalNetSGDOptimizer();","import { Tensor as BaseTensor } from 'causal-net.core';\nimport { platform } from 'causal-net.utils';\nimport { default as functor } from './functor';\nimport { default as SGDFitParamsMixins } from './SGDFitParams.mixins';\nexport default class AdamOptimizer extends platform.mixWith(BaseTensor, \n    [ SGDFitParamsMixins ]){\n    constructor(optimizerParams={}){\n        super();\n        let DefaultParameters = {beta1: 0.1, beta2: 0.2, epsilon:0.03};\n        const {learningRate, beta1, beta2, epsilon} = functor.mergeParams(optimizerParams, DefaultParameters);\n        this.params = { learningRate, beta1, beta2, epsilon };\n        this.trainer = this.T.train.adam(learningRate, beta1, beta2, epsilon);\n    }\n\n    get Params(){\n        return this.params;\n    }\n}","module.exports = __WEBPACK_EXTERNAL_MODULE__7__;","import { Functor as BaseFunctor } from 'causal-net.core';\nclass Functor extends BaseFunctor{\n    constructor(){\n        super();\n    }\n    \n    mergeParams(optimizerParams, defaultOptimizerParameters){\n        const R = this.R;\n        return R.mergeDeepLeft(optimizerParams, defaultOptimizerParameters);\n    }\n}\n\nexport default new Functor();","const TrainerMixins = (BasePipelineClass)=> class extends BasePipelineClass{\n    \n    get Trainer(){\n        const T = this.T;\n        const Loss=this.LossModel, Optimizer=this.Optimizer;\n        return (sampleTensor, labelTensor)=>{\n            const LossFn = ()=>{\n                return T.tidy( ()=>{ return Loss(sampleTensor, labelTensor); } );\n            };\n            return Optimizer.fit(LossFn);\n        };\n    }\n\n    get Optimizer(){\n        if(!this.optimizer){\n            throw Error('optimizer is not set');\n        }\n        return this.optimizer;\n    }\n    \n\n    set Optimizer(optimizer){\n        this.optimizer = optimizer;\n    }\n\n    get TrainDataGenerator(){\n        if(!this.trainDataGenerator){\n            throw Error('TrainDataGenerator is not set');\n        }\n        return this.trainDataGenerator;\n    }\n    set TrainDataGenerator(TrainDataGenerator){\n        this.trainDataGenerator = TrainDataGenerator;\n    }\n\n    async train(numEpochs, batchSize){\n        \n        const F = this.F, R = this.F.CoreFunctor, T = this.T;\n        const TrainDataGenerator = this.TrainDataGenerator, Trainer = this.Trainer;\n        let losses = [], logger = this.Logger;\n        return new Promise(async (resolve, reject)=>{\n            logger.progressBegin(numEpochs);\n            for(let epochIdx of F.range(numEpochs)){\n                const trainData = TrainDataGenerator(batchSize);\n                let iterLosses = [];\n                for await (let { samples, labels } of trainData){\n                    let sampleTensor = T.tensor(samples).asType('float32');\n                    let labelTensor = T.tensor(labels).asType('float32');\n                    let loss = Trainer(sampleTensor, labelTensor);\n                    iterLosses.push(await loss.data());\n                }\n                losses.push(R.mean(iterLosses));\n                iterLosses = [];\n                logger.progressUpdate({epochIdx, losses, numEpochs});\n            }\n            logger.progressEnd();\n            resolve({losses});\n        });\n    }\n\n\n    setByConfig(pipelineConfig){\n        if(super.setByConfig){\n            super.setByConfig(pipelineConfig);\n        }\n        this.Logger.groupBegin('set Trainer by config');\n        const { Optimizer } = pipelineConfig.Net;\n        this.Optimizer = Optimizer;\n        Optimizer.LayerRunner = this.LayerRunner;\n        this.TrainDataGenerator = pipelineConfig.Dataset.TrainDataGenerator;\n        this.Logger.groupEnd();\n    }\n};\n\nexport default TrainerMixins;","const EvaluatorMixins = (BasePipelineClass)=> class extends BasePipelineClass{\n    \n    get TestDataGenerator(){\n        if(!this.testDataGenerator){\n            throw Error('testDataGenerator is not set');\n        }\n        return this.testDataGenerator;\n    }\n    set TestDataGenerator(testDataGenerator){\n        this.testDataGenerator = testDataGenerator;\n    }\n\n    async test(batchSize=32){\n        const T = this.T, R = this.F.CoreFunctor;\n        const TestDataGenerator = this.TestDataGenerator, OneHotPredictModel = this.OneHotPredictModel;\n        return new Promise(async (resolve, reject)=>{\n            const testData = TestDataGenerator(batchSize);    \n            let pass = [];\n            for await (let { samples, labels } of testData){\n                let sampleTensor = T.tensor(samples).asType('float32');\n                let labelTensor = T.tensor(labels).asType('float32');\n                let predictLabelTensor = OneHotPredictModel(sampleTensor);\n                let correctPredicts = predictLabelTensor.mul(labelTensor);\n                pass = [...pass, ... await correctPredicts.sum(1).data()];\n            }\n            let accuracy = R.mean(pass);\n            resolve({accuracy, pass});     \n        });\n    }\n\n\n    setByConfig(pipelineConfig){\n        if(super.setByConfig){\n            super.setByConfig(pipelineConfig);\n        }\n        this.Logger.groupBegin('set Evaluator by config');\n        this.TestDataGenerator = pipelineConfig.Dataset.TestDataGenerator;\n        this.Logger.groupEnd();\n    }\n};\n\nexport default EvaluatorMixins;"],"sourceRoot":""}