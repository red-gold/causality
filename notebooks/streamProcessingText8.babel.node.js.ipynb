{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials\n",
    "\n",
    "## Stream processing with text8 data\n",
    "\n",
    "Input raw text8 corpus file and return the occurent number of each tokens in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use strict'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import * as Preprocessing from 'causal-net.preprocessing';\n",
    "import * as Log from 'causal-net.log';\n",
    "import * as Utils from 'causal-net.utils';\n",
    "import * as Storage from 'causal-net.storage';\n",
    "import * as fs from 'fs';\n",
    "var { indexDBStorage } = Storage;\n",
    "var { stream } = Utils;\n",
    "var { termLogger } = Log;\n",
    "var { nlpPreprocessing, tokenizerEN } = Preprocessing;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create stream process\n",
    "- read chunks from file.\n",
    "- transform each chunk.\n",
    "- write transformed chunk into new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream performance: begin at Fri Mar 15 2019 16:42:45 GMT+0700 (Indochina Time)\n",
      "stream performance: end after 8514 (ms)\n",
      "{ result: 'Success', characterCount: 100000000 }\n"
     ]
    }
   ],
   "source": [
    "var remainingChars = '', wordFreqCount = {}, lineIndex = 0;\n",
    "function tranformFn(chunkData, chunkEncoding, afterTransformFn){\n",
    "    let sampleText = chunkData + remainingChars;\n",
    "    let sampleLines = sampleText.split('\\n');\n",
    "    let transformedData = [];\n",
    "    for(let line of sampleLines){\n",
    "        let tokens = tokenizerEN.tokenize(line);\n",
    "        wordFreqCount = nlpPreprocessing.wordFreqCount(tokens, wordFreqCount);\n",
    "        lineIndex += 1;\n",
    "        transformedData.push({lineIndex, tokens});\n",
    "    }\n",
    "    afterTransformFn(null, transformedData);\n",
    "};\n",
    "var transformer = stream.makeTransform(tranformFn);\n",
    "\n",
    "function writeTokens(transformedData, chunkEncoding, afterWriteFn){\n",
    "    const WriteTokensToFile = async (transformedData)=>{\n",
    "        for(let {lineIndex, tokens} of transformedData){\n",
    "//             console.log({lineIndex});\n",
    "            await indexDBStorage.writeFile(`/corpus/line_${lineIndex}`, JSON.stringify(tokens));\n",
    "        }\n",
    "    }\n",
    "    WriteTokensToFile(transformedData).then(()=>{\n",
    "        afterWriteFn();\n",
    "    })\n",
    "}\n",
    "var writer = stream.makeWritable(writeTokens);\n",
    "var characterCount = 0;\n",
    "(async ()=>{\n",
    "    var corpusReader = fs.createReadStream('../datasets/text8/text8.txt');\n",
    "    const CorpusStreamer = stream.makePipeline([corpusReader, transformer, writer], (data)=>{\n",
    "        characterCount += data.length;\n",
    "    });\n",
    "    termLogger.groupBegin('stream performance');\n",
    "    let result = await CorpusStreamer;\n",
    "    termLogger.groupEnd()\n",
    "    termLogger.log({ result, characterCount } );\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'show 100 items':\n",
      "   [ [ 'anarchism', 303 ],\n",
      "     [ 'originated', 572 ],\n",
      "     [ 'as', 131819 ],\n",
      "     [ 'a', 325895 ],\n",
      "     [ 'term', 7220 ],\n",
      "     [ 'of', 593676 ],\n",
      "     [ 'abuse', 563 ],\n",
      "     [ 'first', 28809 ],\n",
      "     [ 'used', 22736 ],\n",
      "     [ 'against', 8431 ],\n",
      "     [ 'early', 10172 ],\n",
      "     [ 'working', 2270 ],\n",
      "     [ 'class', 3412 ],\n",
      "     [ 'radicals', 116 ],\n",
      "     [ 'including', 9630 ],\n",
      "     [ 'the', 1061363 ],\n",
      "     [ 'diggers', 25 ],\n",
      "     [ 'english', 11868 ],\n",
      "     [ 'revolution', 2029 ],\n",
      "     [ 'and', 416615 ],\n",
      "     [ 'sans', 68 ],\n",
      "     [ 'culottes', 6 ],\n",
      "     [ 'french', 8736 ],\n",
      "     [ 'whilst', 481 ],\n",
      "     [ 'is', 183158 ],\n",
      "     [ 'still', 7378 ],\n",
      "     [ 'in', 372203 ],\n",
      "     [ 'pejorative', 114 ],\n",
      "     [ 'way', 6432 ],\n",
      "     [ 'to', 316375 ],\n",
      "     [ 'describe', 1352 ],\n",
      "     [ 'any', 11804 ],\n",
      "     [ 'act', 3502 ],\n",
      "     [ 'that', 109508 ],\n",
      "     [ 'violent', 653 ],\n",
      "     [ 'means', 4165 ],\n",
      "     [ 'destroy', 466 ],\n",
      "     [ 'organization', 2374 ],\n",
      "     [ 'society', 4067 ],\n",
      "     [ 'it', 73335 ],\n",
      "     [ 'has', 37865 ],\n",
      "     [ 'also', 44358 ],\n",
      "     [ 'been', 25381 ],\n",
      "     [ 'taken', 3043 ],\n",
      "     [ 'up', 12446 ],\n",
      "     [ 'positive', 1254 ],\n",
      "     [ 'label', 646 ],\n",
      "     [ 'by', 111829 ],\n",
      "     [ 'self', 2879 ],\n",
      "     [ 'defined', 2449 ],\n",
      "     [ 'anarchists', 203 ],\n",
      "     [ 'word', 5678 ],\n",
      "     [ 'derived', 1701 ],\n",
      "     [ 'from', 72865 ],\n",
      "     [ 'greek', 4577 ],\n",
      "     [ 'without', 5660 ],\n",
      "     [ 'archons', 10 ],\n",
      "     [ 'ruler', 617 ],\n",
      "     [ 'chief', 2130 ],\n",
      "     [ 'king', 7457 ],\n",
      "     [ 'political', 6967 ],\n",
      "     [ 'philosophy', 2758 ],\n",
      "     [ 'belief', 1572 ],\n",
      "     [ 'rulers', 687 ],\n",
      "     [ 'are', 76523 ],\n",
      "     [ 'unnecessary', 146 ],\n",
      "     [ 'should', 5113 ],\n",
      "     [ 'be', 61283 ],\n",
      "     [ 'abolished', 399 ],\n",
      "     [ 'although', 9286 ],\n",
      "     [ 'there', 22706 ],\n",
      "     [ 'differing', 231 ],\n",
      "     [ 'interpretations', 395 ],\n",
      "     [ 'what', 8581 ],\n",
      "     [ 'this', 58827 ],\n",
      "     [ 'refers', 1570 ],\n",
      "     [ 'related', 3535 ],\n",
      "     [ 'social', 4307 ],\n",
      "     [ 'movements', 1002 ],\n",
      "     [ 'advocate', 331 ],\n",
      "     [ 'elimination', 216 ],\n",
      "     [ 'authoritarian', 185 ],\n",
      "     [ 'institutions', 1021 ],\n",
      "     [ 'particularly', 2881 ],\n",
      "     [ 'state', 12905 ],\n",
      "     [ 'anarchy', 109 ],\n",
      "     [ 'most', 25562 ],\n",
      "     [ 'use', 14011 ],\n",
      "     [ 'does', 5220 ],\n",
      "     [ 'not', 44030 ],\n",
      "     [ 'imply', 257 ],\n",
      "     [ 'chaos', 331 ],\n",
      "     [ 'nihilism', 42 ],\n",
      "     [ 'or', 68948 ],\n",
      "     [ 'anomie', 7 ],\n",
      "     [ 'but', 35356 ],\n",
      "     [ 'rather', 4605 ],\n",
      "     [ 'harmonious', 28 ],\n",
      "     [ 'anti', 3103 ],\n",
      "     [ 'place', 5345 ] ] }\n"
     ]
    }
   ],
   "source": [
    "termLogger.log({'show 100 items': Object.entries(wordFreqCount).slice(0,100)});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, data is saved into files under `/copus/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get list of preprocessing files: begin at Fri Mar 15 2019 16:42:56 GMT+0700 (Indochina Time)\n",
      "get list of preprocessing files: end after 194 (ms)\n",
      "read one file from indexDB: begin at Fri Mar 15 2019 16:42:56 GMT+0700 (Indochina Time)\n",
      "read one file from indexDB: end after 0 (ms)\n",
      "[ 3228, 1293 ]\n"
     ]
    }
   ],
   "source": [
    "(async ()=>{\n",
    "    termLogger.groupBegin('get list of preprocessing files')\n",
    "    let listFiles = await indexDBStorage.getFileList('/corpus/');\n",
    "    termLogger.groupEnd()\n",
    "    termLogger.groupBegin('read one file from indexDB')\n",
    "    let tokens = await indexDBStorage.readFile(listFiles[0]);\n",
    "    termLogger.groupEnd()\n",
    "    termLogger.log([ listFiles.length , JSON.parse(tokens).length]);\n",
    "})()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jp-Babel (Node.js)",
   "language": "babel",
   "name": "babel"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "11.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
