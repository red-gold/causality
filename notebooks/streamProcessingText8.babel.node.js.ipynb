{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials\n",
    "\n",
    "## Stream processing with text8 data\n",
    "\n",
    "Input raw text8 corpus file and return the occurent number of each tokens in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import * as Preprocessing from 'causal-net.preprocessing';\n",
    "import * as Log from 'causal-net.log';\n",
    "import * as Utils from 'causal-net.utils';\n",
    "import * as Storage from 'causal-net.storage';\n",
    "import * as fs from 'fs';\n",
    "var { indexDBStorage } = Storage;\n",
    "var { stream } = Utils;\n",
    "var { termLogger } = Log;\n",
    "var { nlpPreprocessing, tokenizer } = Preprocessing;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create stream process\n",
    "- read chunks from file.\n",
    "- transform each chunk.\n",
    "- write transformed chunks into new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "stream performance: begin at Thu May 09 2019 15:17:07 GMT+0700 (Indochina Time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TypeError: Cannot read property 'tokenize' of undefined\n",
      "    at Transform.tranformFn [as _transform] (evalmachine.<anonymous>:11:30)\n",
      "    at Transform.../../node_modules/readable-stream/lib/_stream_transform.js.Transform._read (/home/huynhnguyen/github/causality/packages/causality-utils/dist/@causalNet/utils.node.js:1746:10)\n",
      "    at Transform.../../node_modules/readable-stream/lib/_stream_transform.js.Transform._write (/home/huynhnguyen/github/causality/packages/causality-utils/dist/@causalNet/utils.node.js:1734:83)\n",
      "    at doWrite (/home/huynhnguyen/github/causality/packages/causality-utils/dist/@causalNet/utils.node.js:2215:64)\n",
      "    at writeOrBuffer (/home/huynhnguyen/github/causality/packages/causality-utils/dist/@causalNet/utils.node.js:2204:5)\n",
      "    at Transform.../../node_modules/readable-stream/lib/_stream_writable.js.Writable.write (/home/huynhnguyen/github/causality/packages/causality-utils/dist/@causalNet/utils.node.js:2121:11)\n",
      "    at ReadStream.ondata (_stream_readable.js:670:20)\n",
      "    at ReadStream.emit (events.js:187:15)\n",
      "    at addChunk (_stream_readable.js:287:12)\n",
      "    at readableAddChunk (_stream_readable.js:268:11)"
     ]
    }
   ],
   "source": [
    "var remainingChars = '', wordFreqCount = {}, lineIndex = 0;\n",
    "function tranformFn(chunkData, chunkEncoding, afterTransformFn){\n",
    "    let sampleText = chunkData + remainingChars;\n",
    "    let sampleLines = sampleText.split('\\n');\n",
    "    let transformedData = [];\n",
    "    for(let line of sampleLines){\n",
    "        let tokens = tokenizer.tokenize(line);\n",
    "        wordFreqCount = nlpPreprocessing.wordFreqCount(tokens, wordFreqCount);\n",
    "        lineIndex += 1;\n",
    "        transformedData.push({lineIndex, tokens});\n",
    "    }\n",
    "    afterTransformFn(null, transformedData);\n",
    "};\n",
    "var transformer = stream.makeTransform(tranformFn);\n",
    "\n",
    "function writeTokens(transformedData, chunkEncoding, afterWriteFn){\n",
    "    const WriteTokensToFile = async (transformedData)=>{\n",
    "        for(let {lineIndex, tokens} of transformedData){\n",
    "//             console.log({lineIndex});\n",
    "            await indexDBStorage.writeFile(`/corpus/line_${lineIndex}`, JSON.stringify(tokens));\n",
    "        }\n",
    "    }\n",
    "    WriteTokensToFile(transformedData).then(()=>{\n",
    "        afterWriteFn();\n",
    "    })\n",
    "}\n",
    "var writer = stream.makeWritable(writeTokens);\n",
    "var characterCount = 0;\n",
    "(async ()=>{\n",
    "    var corpusReader = fs.createReadStream('../server/datasets/text8/text8.txt');\n",
    "    const CorpusStreamer = stream.makePipeline([corpusReader, transformer, writer], (data)=>{\n",
    "        characterCount += data.length;\n",
    "    });\n",
    "    termLogger.groupBegin('stream performance');\n",
    "    let result = await CorpusStreamer;\n",
    "    termLogger.groupEnd()\n",
    "    termLogger.log({ result, characterCount } );\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termLogger.log({'show 100 items': Object.entries(wordFreqCount).slice(0,100)});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, data is saved into files under `/copus/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(async ()=>{\n",
    "    termLogger.groupBegin('get list of preprocessing files')\n",
    "    let listFiles = await indexDBStorage.getFileList('/corpus/');\n",
    "    termLogger.groupEnd()\n",
    "    termLogger.groupBegin('read one file from indexDB')\n",
    "    let tokens = await indexDBStorage.readFile(listFiles[0]);\n",
    "    termLogger.groupEnd()\n",
    "    termLogger.log([ listFiles.length , JSON.parse(tokens).length]);\n",
    "})()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jp-Babel (Node.js)",
   "language": "babel",
   "name": "babel"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "11.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
