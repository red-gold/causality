{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow backend was already registered. Reusing existing backend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'use strict'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import * as Utils from 'causal-net.utils';\n",
    "import * as Log from 'causal-net.log';\n",
    "import * as Storage from 'causal-net.storage';\n",
    "import { causalNetCore } from 'causal-net.core';\n",
    "import * as Preprocessing from 'causal-net.preprocessing';\n",
    "import * as Memory from 'causal-net.memory';\n",
    "import { causalNetSGDOptimizer } from 'causal-net.optimizers';\n",
    "import { causalNetModels } from \"causal-net.models\";\n",
    "import * as Sampling from 'causal-net.sampling';\n",
    "import * as fs from 'fs';\n",
    "var adam = causalNetSGDOptimizer.adam({learningRate: 0.01});\n",
    "var R = causalNetCore.CoreFunction;\n",
    "var T = causalNetCore.CoreTensor;\n",
    "var { Loss } = causalNetModels.skipGram();\n",
    "var { Stream } = Utils;\n",
    "var { termLogger } = Log;\n",
    "var { indexDBStorage } = Storage;\n",
    "var { nlpPreprocessing } = Preprocessing;\n",
    "var { causalNetMemory } = Memory;\n",
    "var { causalNetSampling } = Sampling; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use strict'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function WordCoOccurentTraining(W, Wpos, Wneg, Vectors){\n",
    "    var nce = ()=>{\n",
    "        let Wt = W.map(w=>Vectors[w]);\n",
    "        let Wpost = Wpos.map(w=>Vectors[w]);\n",
    "        let Wnegt = Wneg.map(w=>Vectors[w]);\n",
    "            return ((w, pos, neg)=>{\n",
    "                let [PosLen, NegLen] = [pos.length, neg.length];\n",
    "                var Wa = T.concat(Wt);\n",
    "                let [Ws, Size] = Wa.shape;\n",
    "                console.log(Wa.shape, PosLen , NegLen,Size);\n",
    "                Wa = Wa.reshape([PosLen + NegLen,1,Size]);\n",
    "//                 Wa.print();\n",
    "                var Wb = T.concat([ T.concat(pos), T.concat(neg) ]);\n",
    "//                 Wb.print();\n",
    "                Wb = Wb.reshape([ PosLen + NegLen, Size, 1]);\n",
    "                var label = T.concat([T.ones([PosLen]), T.ones([NegLen]).neg()]);\n",
    "                return Loss(Wa.matMul(Wb).reshape([PosLen + NegLen]), label).neg();\n",
    "                })(Wt, Wpost, Wnegt);\n",
    "            };\n",
    "    return adam.fit(nce);\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5, 5 ] 3 2 5\n",
      "Tensor\n",
      "    22\n",
      "Tensor\n",
      "     [[0.9907162, 1.9903426, 2.9902112, 3.9901443, 4.9901037],]\n",
      "Tensor\n",
      "     [[1, 2, 3, 4, 5],]\n",
      "Tensor\n",
      "     [[0.9907162, 1.9903426, 2.9902112, 3.9901443, 4.9901037],]\n"
     ]
    }
   ],
   "source": [
    "(async ()=>{\n",
    "    var vector = {0: T.variable(T.tensor([1,2,3,4,5]).reshape([1, 5])),\n",
    "                  1: T.variable(T.tensor([1,2,3,4,5]).reshape([1, 5])),\n",
    "                  2: T.variable(T.tensor([1,2,3,4,5]).reshape([1, 5]))}\n",
    "    var W =    [0, 0, 0, 0, 0];\n",
    "    var Wc1 =  [1, 1, 1]; \n",
    "    var Wnc1 = [2, 2];\n",
    "    WordCoOccurentTraining(W, Wc1, Wnc1, vector).print();\n",
    "    for(let v of Object.keys(vector)){\n",
    "        vector[v].print();   \n",
    "    }\n",
    "//     const CheckResult = async ()=>{\n",
    "//             let targetWords = [0,1,2,3,4,5];\n",
    "//             let topKTensor = await memory.getTopKSimilar(targetWords, 10);\n",
    "//             topKTensor.print();    \n",
    "//     }\n",
    "//     await CheckResult();     \n",
    "})();    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var memory;\n",
    "(async ()=>{\n",
    "    memory = causalNetMemory;\n",
    "    let initTensor = await memory.initMemory([15, 2]);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Vocab;\n",
    "(async ()=>{\n",
    "    Vocab = {\n",
    "        words: {'a':0,'b':1,'c':2},\n",
    "        iwords: {'0':'a','1':'b','2':'c'},\n",
    "        wCounts: [1,2,3],\n",
    "        indexToWord: function(idxs){\n",
    "            idxs = Array.from(idxs);//clone avoid bufferArray issues\n",
    "            return idxs.map((idx)=>this.iwords[idx]);\n",
    "        },\n",
    "        wordToIndex: function(ws){\n",
    "            return ws.map((w)=>this.words[w]);\n",
    "        },\n",
    "        countToProb: function(){\n",
    "            let countTotal = R.sum(this.wCounts);\n",
    "            let wFracs = R.map((count)=>count / countTotal, this.wCounts);\n",
    "            this.wProbs = R.map((frac)=>Math.sqrt(frac / 0.001 + 1)*(0.001 / frac), wFracs); \n",
    "            return this.wProbs;\n",
    "        },\n",
    "        samplingNegIndexs: function(positives, size){\n",
    "            return causalNetSampling.negSampling(size, positives, this.wProbs);    \n",
    "        }\n",
    "    };\n",
    "    termLogger.log(Vocab.indexToWord([1,2]));\n",
    "    termLogger.log(Vocab.wordToIndex(['a','b']));\n",
    "    termLogger.log(Vocab.countToProb());\n",
    "    termLogger.log(Vocab.samplingNegIndexs([1,2,3], 6));\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(async ()=>{\n",
    "    let initTensor = await memory.initMemory([10, 5]);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(async ()=>{  \n",
    "    for(let epoch in R.range(0, 5)){\n",
    "        termLogger.log({epoch});\n",
    "           let uids = R.range(0,5);\n",
    "           let nuid = R.range(0,5);\n",
    "           let nvid = R.range(5,9);\n",
    "           uids.map(id=>R.range)\n",
    "           let vecId = new Set([...uids, ...nuid, nvid]);\n",
    "           for(let u in uids){\n",
    "                let uTensor = [];\n",
    "                for(let nu of ){\n",
    "                    \n",
    "                }\n",
    "                await memory.readSlots([u]);\n",
    "                let nuTensor = await memory.readSlots(nu);\n",
    "                let nvTensor = await memory.readSlots(nv);\n",
    "        //         uTensor.print();\n",
    "                WordCoOccurentTraining(uTensor, nuTensor, nvTensor);\n",
    "        //         uTensor.print();\n",
    "                await memory.writeSlots([u], uTensor);\n",
    "                await memory.writeSlots(nu, nuTensor);\n",
    "                await memory.writeSlots(nv, nvTensor);\n",
    "            }\n",
    "            for(let u in R.range(5,9)){\n",
    "                let nu = R.range(5,9);\n",
    "                let nv = R.range(0,5);\n",
    "                let uTensor = await memory.readSlots([u]);\n",
    "                let nuTensor = await memory.readSlots(nu);\n",
    "                let nvTensor = await memory.readSlots(nv);\n",
    "                WordCoOccurentTraining(uTensor, nuTensor, nvTensor);\n",
    "                await memory.writeSlots([u], uTensor);\n",
    "                await memory.writeSlots(nu, nuTensor);\n",
    "                await memory.writeSlots(nv, nvTensor);\n",
    "            }\n",
    "   \n",
    "    }\n",
    "    const CheckResult = async ()=>{\n",
    "        let targetWords = [0,1,2,3,4];\n",
    "        let norm = await memory.normalize();\n",
    "//                     norm.print();\n",
    "        let matchScore = await memory.getMatchScore(targetWords);\n",
    "//                     matchScore.print();\n",
    "        let topKTensor = await memory.getTopKSimilar(targetWords, 5);\n",
    "        topKTensor.print();    \n",
    "    };\n",
    "    await CheckResult();   \n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var remainingChars = '', wordFreqCount = {}, lineIndex = 0;\n",
    "function tranformFn(chunkData, chunkEncoding, afterTransformFn){\n",
    "    let sampleText = remainingChars + chunkData;\n",
    "    let sampleLines = sampleText.split('\\n');\n",
    "    let transformedData = [], counter = 0;\n",
    "    for(let line of sampleLines){\n",
    "        counter += 1;\n",
    "        if(counter === sampleLines.length){//last line\n",
    "            remainingChars = line;\n",
    "        }\n",
    "        else{\n",
    "            if(line.length > 0){\n",
    "                let tokens = nlpPreprocessing.tokenize(line);\n",
    "                wordFreqCount = nlpPreprocessing.WordFreqCount(tokens, wordFreqCount);\n",
    "                lineIndex += 1;\n",
    "                //console.log({line, lineIndex, tokens});\n",
    "                transformedData.push({lineIndex, tokens});\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    afterTransformFn(null, transformedData);\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function writeTokens(transformedData, chunkEncoding, afterWriteFn){\n",
    "    const WriteTokensToFile = async (transformedData)=>{\n",
    "        for(let {lineIndex, tokens} of transformedData){\n",
    "            console.log({lineIndex});\n",
    "            await indexDBStorage.writeFile(`/corpus/line_${lineIndex}`, JSON.stringify(tokens));\n",
    "        }\n",
    "    }\n",
    "    WriteTokensToFile(transformedData).then(()=>{\n",
    "        afterWriteFn();\n",
    "    });\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var allTokens = [], tokenMatrix = null, Words=[], wordCounts=[];\n",
    "(async ()=>{\n",
    "    var corpusReader = fs.createReadStream('../datasets/text8/text8.txt');\n",
    "    let writer = Stream.makeWritable(writeTokens);\n",
    "    let transformer = Stream.makeTransform(tranformFn);\n",
    "    let deletedFiles = await indexDBStorage.deleteFileByPrefix('/corpus/');\n",
    "    termLogger.log({deletedFiles});\n",
    "    const DataProgress = (dataBuffer)=> termLogger.log({'data length': dataBuffer.length});\n",
    "    const CorpusStreamer = Stream.makePipeline([corpusReader, transformer, writer], DataProgress);\n",
    "    let result = await CorpusStreamer;\n",
    "    \n",
    "    const SortByFreq = R.sortBy(([w,f])=>-f);\n",
    "    let vocabFreqPairs = R.filter(([w,f])=>f>0)(SortByFreq(R.toPairs(wordFreqCount)));\n",
    "    let [choosePairs, filterPairs] = R.splitAt(10000, vocabFreqPairs);  \n",
    "    termLogger.log({'keep length': choosePairs.length, 'discard': filterPairs.length})\n",
    "    const GetVocab = ([v,f]) => v;\n",
    "    const GetFreq = ([v,f]) => f;\n",
    "    Words = R.map(GetVocab, choosePairs);\n",
    "    wordCounts = R.map(GetFreq, choosePairs);\n",
    "})();\n",
    "termLogger.log([Words.length, wordCounts.length]);\n",
    "var wordMapper = R.compose( R.fromPairs, R.addIndex(R.map)((v,i)=>[v, i]) )(Words);\n",
    "var iwordMapper = R.compose( R.fromPairs, R.addIndex(R.map)((v,i)=>[i, v]) )(Words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var wordMapper = R.compose( R.fromPairs, R.addIndex(R.map)((v,i)=>[v, i]) )(Words);\n",
    "var iwordMapper = R.compose( R.fromPairs, R.addIndex(R.map)((v,i)=>[i, v]) )(Words);\n",
    "termLogger.log(Object.keys(wordMapper).length);\n",
    "termLogger.log(Object.keys(iwordMapper).length);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var Vocab = {\n",
    "    words: wordMapper,\n",
    "    iwords: iwordMapper,\n",
    "    wCounts: wordCounts,\n",
    "    indexToWord: function(idxs){\n",
    "        idxs = Array.from(idxs);//clone avoid bufferArray issues\n",
    "        return idxs.map((idx)=>this.iwords[idx]);\n",
    "    },\n",
    "    wordToIndex: function(ws){\n",
    "        return ws.map((w)=>this.words[w]);\n",
    "    },\n",
    "    countToProb: function(){\n",
    "        let countTotal = R.sum(this.wCounts);\n",
    "        console.log(countTotal);\n",
    "        let wFracs = R.map((count)=>count / countTotal, this.wCounts);\n",
    "        //rebalancing the rare words change\n",
    "        this.wProbs = R.map((frac)=>Math.sqrt(frac / 0.001 + 1)*(0.001 / frac), wFracs); \n",
    "        return this.wProbs;\n",
    "    },\n",
    "    samplingNegIndexs: function(positives, size){\n",
    "        return causalNetSampling.negSampling(size, positives, this.wProbs);    \n",
    "    }\n",
    "};\n",
    "var _prob = Vocab.countToProb();\n",
    "_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "termLogger.log(Vocab.indexToWord([0, 1, 2]));\n",
    "termLogger.log(Vocab.wordToIndex(['a','the111']));\n",
    "termLogger.log(Vocab.samplingNegIndexs([1,2,3], 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var MatrixInit = (s)=>R.map(()=>R.map(()=>0)(R.range(0,s)))(R.range(0,s))\n",
    "var CooccurenceMatrixReducer = R.reduce((total, token)=>{\n",
    "    let [w, ctx] = token;\n",
    "    let r = w, c = ctx;\n",
    "    try{\n",
    "       total[r][c] += 1; \n",
    "    }\n",
    "    catch(e){\n",
    "        console.error(token)\n",
    "    }\n",
    "    return total;\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "function getTokenContext(tokens, windownSize){\n",
    "    function context({tokenContext, cooccurents}, token, index){\n",
    "        let leftSizeContext = [], rightSizeContext = [], occur = new Set([token]);\n",
    "        for(let w_idx of R.range(1, windownSize+1)){\n",
    "            if(index - w_idx >= 0){\n",
    "                occur.add(tokens[index - w_idx]);\n",
    "            }\n",
    "            if(index + w_idx < tokens.length){\n",
    "                occur.add(tokens[index + w_idx]);\n",
    "            }\n",
    "        }\n",
    "        tokenContext.push(token);\n",
    "        cooccurents.push(Array.from(occur));\n",
    "        return {tokenContext, cooccurents};\n",
    "    }\n",
    "    let { tokenContext, cooccurents } = R.addIndex(R.reduce)(context,\n",
    "             { tokenContext:[], cooccurents:[] }, tokens);    \n",
    "    \n",
    "    return {tokenContext, cooccurents};\n",
    "}\n",
    "var tokens = [9,2,3,45,5,6];\n",
    "var {tokenContext, cooccurents } = getTokenContext(tokens, 3);\n",
    "console.log(R.zip(tokenContext, cooccurents));\n",
    "var tokens = [9,2,3,45,5,6];\n",
    "var {tokenContext, cooccurents } = getTokenContext(tokens, 3);\n",
    "console.log(R.zip(tokenContext, cooccurents));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(async ()=>{\n",
    "    var VocabLen = Vocab.wCounts.length;\n",
    "    console.log(VocabLen);\n",
    "//     var tokenMatrix = MatrixInit(VocabLen, VocabLen);\n",
    "    await memory.initMemory([VocabLen, 100]);\n",
    "})();       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(async ()=>{    \n",
    "    let uTensor = await memory.readSlots([0]);\n",
    "    let nuTensor = await memory.readSlots([1, 2, 3, 4, 5, 6]);\n",
    "    let nvTensor = await memory.readSlots([8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]);\n",
    "    WordCoOccurentTraining(uTensor, nuTensor, nvTensor);\n",
    "    await memory.writeSlots([0], uTensor);\n",
    "    await memory.writeSlots([1,2], nuTensor);\n",
    "    await memory.writeSlots([8,9], nvTensor);\n",
    "    const CheckResult = async ()=>{\n",
    "        let targetWords = [0,1,2,3,4,5];\n",
    "        let topKTensor = await memory.getTopKSimilar(targetWords, 10);\n",
    "        topKTensor.print();\n",
    "        let topks = R.splitEvery(10, Array.from(await topKTensor.data()));\n",
    "        for(let [w, sim] of R.zip(targetWords, topks)){\n",
    "            let [tw, ...sws] = Vocab.indexToWord(sim);\n",
    "            termLogger.log(`[${tw}] is similar to ${sws.join(',')}`);\n",
    "        }    \n",
    "    }\n",
    "    await CheckResult();\n",
    "})();     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.writeSlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(async ()=>{\n",
    "    let listFiles = await indexDBStorage.getFileList('/corpus/');\n",
    "    let getlist = listFiles.slice(0,1);//take all\n",
    "    let startTime = new Date();\n",
    "    for(let epoch in R.range(0, 100)){\n",
    "        for(let lfile of getlist){\n",
    "            termLogger.log({lfile, elapse: new Date() - startTime});\n",
    "            let rawtokens = await indexDBStorage.readFile(lfile);\n",
    "            let tokens = JSON.parse(rawtokens);\n",
    "            const tokenIdxs = Vocab.wordToIndex(tokens);\n",
    "            \n",
    "            const FilterUndefined = R.filter((v)=>v!==undefined);\n",
    "            let seletedTokens = FilterUndefined(tokenIdxs);\n",
    "            let loss = [];\n",
    "            var {tokenContext, cooccurents} = getTokenContext(seletedTokens, 2);\n",
    "//             console.log(R.zip(tokenContext, cooccurents), seletedTokens);\n",
    "            for(let [w, posWs] of R.zip(tokenContext, cooccurents)){            \n",
    "                \n",
    "                let negWs = Vocab.samplingNegIndexs(posWs, 10);\n",
    "//                 console.log({w, posWs, negWs});\n",
    "                let uTensor = await memory.readSlots([w]);\n",
    "                let nuTensor = await memory.readSlots(posWs);\n",
    "                let nvTensor = await memory.readSlots(negWs);\n",
    "                \n",
    "                let l = await WordCoOccurentTraining(uTensor, nuTensor, nvTensor).data();\n",
    "                loss = [...loss, ...l];\n",
    "                await memory.writeSlots([w], uTensor);\n",
    "                await memory.writeSlots(posWs, nuTensor);\n",
    "                await memory.writeSlots(negWs, nvTensor);\n",
    "            }\n",
    "            console.log({epoch, lfile, loss: R.mean(loss)});\n",
    "        }\n",
    "        const CheckResult = async ()=>{\n",
    "            let targetWords = [0,1,2,3,4,5];\n",
    "            let topKTensor = await memory.getTopKSimilar(targetWords, 10);\n",
    "            topKTensor.print();\n",
    "            let topks = R.splitEvery(10, Array.from(await topKTensor.data()));\n",
    "            for(let [w, sim] of R.zip(targetWords, topks)){\n",
    "                let [tw, ...sws] = Vocab.indexToWord(sim);\n",
    "                termLogger.log(`[${tw}] is similar to ${sws.join(',')}`);\n",
    "            }    \n",
    "        }\n",
    "        await CheckResult();    \n",
    "    }\n",
    "    // tokenMatrix = CooccurenceMatrixReducer(tokenMatrix)(tokenContexts);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.writeFileSync('./cooccurent.matrix.json', JSON.stringify(tokenMatrix));\n",
    "fs.writeFileSync('./tokenMapper.json', JSON.stringify(mapper));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jp-Babel (Node.js)",
   "language": "babel",
   "name": "babel"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "11.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
