{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import * as Utils from 'causal-net.utils';\n",
    "import * as Log from 'causal-net.log';\n",
    "import * as Storage from 'causal-net.storage';\n",
    "import * as Preprocessing from 'causal-net.preprocessing';\n",
    "import * as Memory from 'causal-net.memory';\n",
    "import { causalNetCore } from 'causal-net.core';\n",
    "import { causalNetSGDOptimizer } from 'causal-net.optimizers';\n",
    "import { causalNetModels } from \"causal-net.models\";\n",
    "import * as Sampling from 'causal-net.sampling';\n",
    "import * as fs from 'fs';\n",
    "var R = causalNetCore.CoreFunction;\n",
    "var T = causalNetCore.CoreTensor;\n",
    "var { Loss } = causalNetModels.skipGram();\n",
    "var { Stream } = Utils;\n",
    "var { termLogger } = Log;\n",
    "var { indexDBStorage } = Storage;\n",
    "var { nlpPreprocessing } = Preprocessing;\n",
    "var { causalNetMemory } = Memory;\n",
    "var { causalNetSampling } = Sampling; \n",
    "var optimizers = causalNetSGDOptimizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// var docs = ['He is the king','The king is royal', 'She is the royal queen'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var corpus = '' + fs.readFileSync('../datasets/text8/small.txt')\n",
    "var docs = corpus.split('\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function* skipGramContext(docTokens, windowSize){\n",
    "    for(let tokens of docTokens){\n",
    "        for(let index of R.range(0, tokens.length)){\n",
    "            let target = tokens[index];\n",
    "            for(let wid of R.range(-windowSize, windowSize)){\n",
    "                let context = tokens[index + wid];\n",
    "                if(context !== undefined && target !== context){\n",
    "                    yield [target, context];\n",
    "                }\n",
    "            }\n",
    "        }    \n",
    "    }\n",
    "};\n",
    "var docTokens = [];\n",
    "for(let raw of docs){\n",
    "    docTokens.push(nlpPreprocessing.tokenize(raw));\n",
    "}\n",
    "var vocab = R.compose(R.uniq, R.flatten)(docTokens);\n",
    "var word2int = R.compose(R.fromPairs, R.addIndex(R.map)((w,i)=>[w,i]))(vocab);\n",
    "var int2word = R.compose(R.fromPairs, R.addIndex(R.map)((w,i)=>[i,w]))(vocab);\n",
    "var trainTargets = [], trainContexts = [], trainNegContexts = [];\n",
    "let nonContext = R.filter((x)=>R.indexOf(x, [word2int[target], word2int[context]])===-1)\n",
    "                            (R.range(0, vocab.length));\n",
    "for(let [target, context] of skipGramContext(docTokens, 2)){\n",
    "    \n",
    "//     console.log([target, context, word2int[target], word2int[context], nonContext]);\n",
    "    trainTargets.push(word2int[target]);\n",
    "    trainContexts.push(word2int[context]);\n",
    "    trainNegContexts.push(nonContext);\n",
    "}\n",
    "console.log(vocab.length);\n",
    "console.log(trainNegContexts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var VocabLen = vocab.length, EmbeddingSize = 50;\n",
    "var UVecs = T.variable(T.randomNormal([VocabLen + 1, EmbeddingSize]));\n",
    "var VVecs = T.variable(T.randomNormal([VocabLen + 1, EmbeddingSize]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var adam = optimizers.adam({learningRate: 0.2});\n",
    "function trainLabel(targetId, contextId){\n",
    "    return adam.fit(()=>{\n",
    "        let [UEmbed, Ubias] = UVecs.split([VocabLen, 1], 0);\n",
    "        let [VEmbed, Vbias] = VVecs.split([VocabLen, 1], 0);\n",
    "        let enc = T.oneHot(targetId, VocabLen).matMul(UEmbed).add(Ubias);\n",
    "        let dec = enc.add(Vbias).matMul(VEmbed.transpose());\n",
    "        let logProb = T.oneHot(contextId, VocabLen).mul(dec.sub(dec.logSumExp(1, true))).neg().mean();\n",
    "        return logProb;\n",
    "    }, [UVecs, VVecs]);\n",
    "}\n",
    "for(let epoch of R.range(0,500)){\n",
    "    trainLabel(trainTargets, trainContexts).print();\n",
    "}\n",
    "\n",
    "async function checkTopMatch(words, embedding, k=3){\n",
    "    function normalize(vecs){\n",
    "        let meanTs = vecs.mean(1, true);\n",
    "        let stdTs = vecs.sub(meanTs).pow(2).mean(1, true).pow(0.5);\n",
    "        return vecs.sub(meanTs).div(stdTs);    \n",
    "    }\n",
    "    function getMatchScore(slotIdxs, normVecs){\n",
    "        let cTs = normVecs.gather(slotIdxs);\n",
    "        let similarityScore = normVecs.dot(cTs.transpose());\n",
    "        return similarityScore;\n",
    "    }\n",
    "    var norms = normalize(embedding);\n",
    "    for(let w of words){\n",
    "        let wid = word2int[w];\n",
    "        let matchScores = getMatchScore([wid], norms);\n",
    "        let {values, indices} = matchScores.transpose().topk(k);\n",
    "        let idxs = await indices.data();\n",
    "        let [targetW, ...similarWs] = Array.from(idxs).map(i=>int2word[i]);\n",
    "        console.log(`[${targetW}\\t] is similar to: ${similarWs.join(', ')}`)\n",
    "    }\n",
    "}\n",
    "var embed = UVecs.split([VocabLen, 1], 0)[0];\n",
    "checkTopMatch(vocab, embed, 7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.filter((x)=>R.indexOf(x,[1,2])===-1)([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var adam = optimizers.adam({learningRate: 0.1});\n",
    "function trainLabel(targetId, contextId, negContextId){\n",
    "    console.log(negContextId.length);\n",
    "    return adam.fit(()=>{\n",
    "        let [UEmbed, Ubias] = UVecs.split([VocabLen, 1], 0);\n",
    "        let [VEmbed, Vbias] = VVecs.split([VocabLen, 1], 0);\n",
    "        let posU = UEmbed.gather(targetId).reshape([-1, 1, EmbeddingSize]);\n",
    "        let posV = VEmbed.gather(contextId).reshape([-1, EmbeddingSize, 1]); \n",
    "        let pos = posU.matMul(posV).logSigmoid().mean();\n",
    "        for(let bId of R.range(0, targetId.length)){\n",
    "            let negU = UEmbed.gather(R.repeat(targetId[bId], negContextId[bId].length))\n",
    "                                .reshape([-1, 1, EmbeddingSize]);\n",
    "            let negV = VEmbed.gather(negContextId[bId])\n",
    "                                .reshape([-1, EmbeddingSize, 1]);\n",
    "            pos.add(negU.matMul(negV).neg().logSigmoid().mean());\n",
    "        }\n",
    "        let logProb = pos.neg().mean();\n",
    "        return logProb;\n",
    "    }, [UVecs, VVecs]);\n",
    "}\n",
    "for(let epoch of R.range(0,500)){\n",
    "    trainLabel(trainTargets, trainContexts, trainNegContexts).print();\n",
    "}\n",
    "\n",
    "async function checkTopMatch(words, embedding, k=3){\n",
    "    function normalize(vecs){\n",
    "        let meanTs = vecs.mean(1, true);\n",
    "        let stdTs = vecs.sub(meanTs).pow(2).mean(1, true).pow(0.5);\n",
    "        return vecs.sub(meanTs).div(stdTs);    \n",
    "    }\n",
    "    function getMatchScore(slotIdxs, normVecs){\n",
    "        let cTs = normVecs.gather(slotIdxs);\n",
    "        let similarityScore = normVecs.dot(cTs.transpose());\n",
    "        return similarityScore;\n",
    "    }\n",
    "    var norms = normalize(embedding);\n",
    "    for(let w of words){\n",
    "        let wid = word2int[w];\n",
    "        let matchScores = getMatchScore([wid], norms);\n",
    "        let {values, indices} = matchScores.transpose().topk(k);\n",
    "        let idxs = await indices.data();\n",
    "        let [targetW, ...similarWs] = Array.from(idxs).map(i=>int2word[i]);\n",
    "        console.log(`[${targetW}\\t] is similar to: ${similarWs.join(', ')}`)\n",
    "    }\n",
    "}\n",
    "var embed = UVecs.split([VocabLen, 1], 0)[0];\n",
    "checkTopMatch(vocab, embed, 7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function WordCoOccurentTraining(W, Wpos, Wneg, Vectors){\n",
    "    var nce = ()=>{\n",
    "        let Wpost = Wpos.map(w=>Vectors[w]);\n",
    "        let Wnegt = Wneg.map(w=>Vectors[w]);\n",
    "        let Wt = [...Wpost, ...WnegT].map(()=>Vectors[W]);\n",
    "            return ((w, pos, neg)=>{\n",
    "                let [PosLen, NegLen] = [pos.length, neg.length];\n",
    "                var Wa = T.concat(Wt);\n",
    "                let [Ws, Size] = Wa.shape;\n",
    "                console.log(Wa.shape, PosLen , NegLen,Size);\n",
    "                Wa = Wa.reshape([PosLen + NegLen,1,Size]);\n",
    "//                 Wa.print();\n",
    "                var Wb = T.concat([ T.concat(pos), T.concat(neg) ]);\n",
    "//                 Wb.print();\n",
    "                Wb = Wb.reshape([ PosLen + NegLen, Size, 1]);\n",
    "                var label = T.concat([T.ones([PosLen]), T.ones([NegLen]).neg()]);\n",
    "                return Loss(Wa.matMul(Wb).reshape([PosLen + NegLen]), label).neg();\n",
    "                })(Wt, Wpost, Wnegt);\n",
    "            };\n",
    "    return adam.fit(nce);\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(async ()=>{\n",
    "    var vector = {0: T.variable(T.tensor([1,2,3,4,5]).reshape([1, 5])),\n",
    "                  1: T.variable(T.tensor([1,2,3,4,5]).reshape([1, 5])),\n",
    "                  2: T.variable(T.tensor([1,2,3,4,5]).reshape([1, 5]))}\n",
    "    var W =    [0, 0, 0, 0, 0];\n",
    "    var Wc1 =  [1, 1, 1]; \n",
    "    var Wnc1 = [2, 2];\n",
    "    WordCoOccurentTraining(vids, vector).print();\n",
    "    for(let v of Object.keys(vector)){\n",
    "        vector[v].print();   \n",
    "    }\n",
    "//     const CheckResult = async ()=>{\n",
    "//             let targetWords = [0,1,2,3,4,5];\n",
    "//             let topKTensor = await memory.getTopKSimilar(targetWords, 10);\n",
    "//             topKTensor.print();    \n",
    "//     }\n",
    "//     await CheckResult();     \n",
    "})();    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var memory;\n",
    "(async ()=>{\n",
    "    memory = causalNetMemory;\n",
    "    let initTensor = await memory.initMemory([15, 2]);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Vocab;\n",
    "(async ()=>{\n",
    "    Vocab = {\n",
    "        words: {'a':0,'b':1,'c':2},\n",
    "        iwords: {'0':'a','1':'b','2':'c'},\n",
    "        wCounts: [1,2,3],\n",
    "        indexToWord: function(idxs){\n",
    "            idxs = Array.from(idxs);//clone avoid bufferArray issues\n",
    "            return idxs.map((idx)=>this.iwords[idx]);\n",
    "        },\n",
    "        wordToIndex: function(ws){\n",
    "            return ws.map((w)=>this.words[w]);\n",
    "        },\n",
    "        countToProb: function(){\n",
    "            let countTotal = R.sum(this.wCounts);\n",
    "            let wFracs = R.map((count)=>count / countTotal, this.wCounts);\n",
    "            this.wProbs = R.map((frac)=>Math.sqrt(frac / 0.001 + 1)*(0.001 / frac), wFracs); \n",
    "            return this.wProbs;\n",
    "        },\n",
    "        samplingNegIndexs: function(positives, size){\n",
    "            return causalNetSampling.negSampling(size, positives, this.wProbs);    \n",
    "        }\n",
    "    };\n",
    "    termLogger.log(Vocab.indexToWord([1,2]));\n",
    "    termLogger.log(Vocab.wordToIndex(['a','b']));\n",
    "    termLogger.log(Vocab.countToProb());\n",
    "    termLogger.log(Vocab.samplingNegIndexs([1,2,3], 6));\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(async ()=>{\n",
    "    let initTensor = await memory.initMemory([10, 5]);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(async ()=>{  \n",
    "    for(let epoch in R.range(0, 501)){\n",
    "        termLogger.log({epoch});\n",
    "        let uids = R.range(0,5);\n",
    "        let nuid = R.reverse(R.range(0,5));\n",
    "        let nvid = R.range(5,10);\n",
    "        uids = [...uids, ...uids];\n",
    "        let vecId = new Set([...uids, ...nuid, ...nvid]);\n",
    "        let vectors = {};\n",
    "        for(let v of vecId){\n",
    "            vectors[v] = await memory.readSlots([v]);\n",
    "        }\n",
    "        WordCoOccurentTraining(uids, nuid, nvid, vectors).print();\n",
    "        for(let v of vecId){\n",
    "            await memory.writeSlots([v], vectors[v]);\n",
    "        }\n",
    "        uids = R.range(5,10);\n",
    "        nuid = R.reverse(R.range(5,10));\n",
    "        nvid = R.range(0,5);\n",
    "        uids = [...uids, ...uids];\n",
    "        vecId = new Set([...uids, ...nuid, ...nvid]);\n",
    "        vectors = {};\n",
    "        for(let v of vecId){\n",
    "            vectors[v] = await memory.readSlots([v]);\n",
    "        }\n",
    "        WordCoOccurentTraining(uids, nuid, nvid, vectors).print();\n",
    "        for(let v of vecId){\n",
    "            await memory.writeSlots([v], vectors[v]);\n",
    "        }\n",
    "   \n",
    "    }\n",
    "    const CheckResult = async ()=>{\n",
    "        let targetWords = [0,1,4,3,2];\n",
    "        let norm = await memory.normalize();\n",
    "        //  norm.print();\n",
    "        let matchScore = await memory.getMatchScore(targetWords);\n",
    "        //  matchScore.print();\n",
    "        let topKTensor = await memory.getTopKSimilar(targetWords, 5);\n",
    "        topKTensor.print();    \n",
    "    };\n",
    "    await CheckResult();   \n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var remainingChars = '', wordFreqCount = {}, lineIndex = 0;\n",
    "function tranformFn(chunkData, chunkEncoding, afterTransformFn){\n",
    "    let sampleText = remainingChars + chunkData;\n",
    "    let sampleLines = sampleText.split('\\n');\n",
    "    let transformedData = [], counter = 0;\n",
    "    for(let line of sampleLines){\n",
    "        counter += 1;\n",
    "        if(counter === sampleLines.length){//last line\n",
    "            remainingChars = line;\n",
    "        }\n",
    "        else{\n",
    "            if(line.length > 0){\n",
    "                let tokens = nlpPreprocessing.tokenize(line);\n",
    "                wordFreqCount = nlpPreprocessing.WordFreqCount(tokens, wordFreqCount);\n",
    "                lineIndex += 1;\n",
    "                //console.log({line, lineIndex, tokens});\n",
    "                transformedData.push({lineIndex, tokens});\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    afterTransformFn(null, transformedData);\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function writeTokens(transformedData, chunkEncoding, afterWriteFn){\n",
    "    const WriteTokensToFile = async (transformedData)=>{\n",
    "        for(let {lineIndex, tokens} of transformedData){\n",
    "            console.log({lineIndex});\n",
    "            await indexDBStorage.writeFile(`/corpus/line_${lineIndex}`, JSON.stringify(tokens));\n",
    "        }\n",
    "    }\n",
    "    WriteTokensToFile(transformedData).then(()=>{\n",
    "        afterWriteFn();\n",
    "    });\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var allTokens = [], tokenMatrix = null, Words=[], wordCounts=[];\n",
    "(async ()=>{\n",
    "    var corpusReader = fs.createReadStream('../datasets/text8/text8.txt');\n",
    "    let writer = Stream.makeWritable(writeTokens);\n",
    "    let transformer = Stream.makeTransform(tranformFn);\n",
    "    let deletedFiles = await indexDBStorage.deleteFileByPrefix('/corpus/');\n",
    "    termLogger.log({deletedFiles});\n",
    "    const DataProgress = (dataBuffer)=> termLogger.log({'data length': dataBuffer.length});\n",
    "    const CorpusStreamer = Stream.makePipeline([corpusReader, transformer, writer], DataProgress);\n",
    "    let result = await CorpusStreamer;\n",
    "    \n",
    "    const SortByFreq = R.sortBy(([w,f])=>-f);\n",
    "    let vocabFreqPairs = R.filter(([w,f])=>f>0)(SortByFreq(R.toPairs(wordFreqCount)));\n",
    "    let [choosePairs, filterPairs] = R.splitAt(10000, vocabFreqPairs);  \n",
    "    termLogger.log({'keep length': choosePairs.length, 'discard': filterPairs.length})\n",
    "    const GetVocab = ([v,f]) => v;\n",
    "    const GetFreq = ([v,f]) => f;\n",
    "    Words = R.map(GetVocab, choosePairs);\n",
    "    wordCounts = R.map(GetFreq, choosePairs);\n",
    "})();\n",
    "termLogger.log([Words.length, wordCounts.length]);\n",
    "var wordMapper = R.compose( R.fromPairs, R.addIndex(R.map)((v,i)=>[v, i]) )(Words);\n",
    "var iwordMapper = R.compose( R.fromPairs, R.addIndex(R.map)((v,i)=>[i, v]) )(Words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var wordMapper = R.compose( R.fromPairs, R.addIndex(R.map)((v,i)=>[v, i]) )(Words);\n",
    "var iwordMapper = R.compose( R.fromPairs, R.addIndex(R.map)((v,i)=>[i, v]) )(Words);\n",
    "termLogger.log(Object.keys(wordMapper).length);\n",
    "termLogger.log(Object.keys(iwordMapper).length);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var Vocab = {\n",
    "    words: wordMapper,\n",
    "    iwords: iwordMapper,\n",
    "    wCounts: wordCounts,\n",
    "    indexToWord: function(idxs){\n",
    "        idxs = Array.from(idxs);//clone avoid bufferArray issues\n",
    "        return idxs.map((idx)=>this.iwords[idx]);\n",
    "    },\n",
    "    wordToIndex: function(ws){\n",
    "        return ws.map((w)=>this.words[w]);\n",
    "    },\n",
    "    countToProb: function(){\n",
    "        let countTotal = R.sum(this.wCounts);\n",
    "        console.log(countTotal);\n",
    "        let wFracs = R.map((count)=>count / countTotal, this.wCounts);\n",
    "        //rebalancing for rare words\n",
    "        this.wProbs = R.map((frac)=>Math.sqrt(frac / 0.001 + 1)*(0.001 / frac), wFracs); \n",
    "        return this.wProbs;\n",
    "    },\n",
    "    samplingNegIndexs: function(positives, size){\n",
    "        return causalNetSampling.negSampling(size, positives, this.wProbs);    \n",
    "    }\n",
    "};\n",
    "var _prob = Vocab.countToProb();\n",
    "_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "termLogger.log(Vocab.indexToWord([0, 1, 2]));\n",
    "termLogger.log(Vocab.wordToIndex(['a','the111']));\n",
    "termLogger.log(Vocab.samplingNegIndexs([1,2,3], 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var MatrixInit = (s)=>R.map(()=>R.map(()=>0)(R.range(0,s)))(R.range(0,s))\n",
    "var CooccurenceMatrixReducer = R.reduce((total, token)=>{\n",
    "    let [w, ctx] = token;\n",
    "    let r = w, c = ctx;\n",
    "    try{\n",
    "       total[r][c] += 1; \n",
    "    }\n",
    "    catch(e){\n",
    "        console.error(token)\n",
    "    }\n",
    "    return total;\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "function getTokenContext(tokens, windownSize, batchSize=2){\n",
    "    function context({tokenContext, cooccurents}, token, index){\n",
    "        let leftSizeContext = [], rightSizeContext = [], occur = new Set([token]);\n",
    "        for(let w_idx of R.range(1, windownSize+1)){\n",
    "            if(index - w_idx >= 0){\n",
    "                occur.add(tokens[index - w_idx]);\n",
    "            }\n",
    "            if(index + w_idx < tokens.length){\n",
    "                occur.add(tokens[index + w_idx]);\n",
    "            }\n",
    "        }\n",
    "        tokenContext.push(token);\n",
    "        cooccurents.push(Array.from(occur));\n",
    "        return {tokenContext, cooccurents};\n",
    "    }\n",
    "    let { tokenContext, cooccurents } = R.addIndex(R.reduce)(context,\n",
    "             { tokenContext:[], cooccurents:[] }, tokens);    \n",
    "    \n",
    "    return {tokenContext, cooccurents};\n",
    "}\n",
    "var tokens = [9,2,3,45,5,6];\n",
    "var {tokenContext, cooccurents } = getTokenContext(tokens, 3);\n",
    "console.log(R.zip(tokenContext, cooccurents));\n",
    "var tokens = [9,2,3,45,5,6];\n",
    "var {tokenContext, cooccurents } = getTokenContext(tokens, 3);\n",
    "console.log(R.zip(tokenContext, cooccurents));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(async ()=>{\n",
    "    var VocabLen = Vocab.wCounts.length;\n",
    "    console.log(VocabLen);\n",
    "//     var tokenMatrix = MatrixInit(VocabLen, VocabLen);\n",
    "    await memory.initMemory([VocabLen, 100]);\n",
    "})();       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(async ()=>{    \n",
    "    let uTensor = await memory.readSlots([0]);\n",
    "    let nuTensor = await memory.readSlots([1, 2, 3, 4, 5, 6]);\n",
    "    let nvTensor = await memory.readSlots([8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]);\n",
    "    WordCoOccurentTraining(uTensor, nuTensor, nvTensor);\n",
    "    await memory.writeSlots([0], uTensor);\n",
    "    await memory.writeSlots([1,2], nuTensor);\n",
    "    await memory.writeSlots([8,9], nvTensor);\n",
    "    const CheckResult = async ()=>{\n",
    "        let targetWords = [0,1,2,3,4,5];\n",
    "        let topKTensor = await memory.getTopKSimilar(targetWords, 10);\n",
    "        topKTensor.print();\n",
    "        let topks = R.splitEvery(10, Array.from(await topKTensor.data()));\n",
    "        for(let [w, sim] of R.zip(targetWords, topks)){\n",
    "            let [tw, ...sws] = Vocab.indexToWord(sim);\n",
    "            termLogger.log(`[${tw}] is similar to ${sws.join(',')}`);\n",
    "        }    \n",
    "    }\n",
    "    await CheckResult();\n",
    "})();     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.writeSlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(async ()=>{\n",
    "    let listFiles = await indexDBStorage.getFileList('/corpus/');\n",
    "    let getlist = listFiles.slice(0,1);//take all\n",
    "    let startTime = new Date();\n",
    "    for(let epoch in R.range(0, 100)){\n",
    "        for(let lfile of getlist){\n",
    "            termLogger.log({lfile, elapse: new Date() - startTime});\n",
    "            let rawtokens = await indexDBStorage.readFile(lfile);\n",
    "            let tokens = JSON.parse(rawtokens);\n",
    "            const tokenIdxs = Vocab.wordToIndex(tokens);\n",
    "            \n",
    "            const FilterUndefined = R.filter((v)=>v!==undefined);\n",
    "            let seletedTokens = FilterUndefined(tokenIdxs);\n",
    "            let loss = [];\n",
    "            var {tokenContext, cooccurents} = getTokenContext(seletedTokens, 2);\n",
    "//             console.log(R.zip(tokenContext, cooccurents), seletedTokens);\n",
    "            for(let [w, posWs] of R.zip(tokenContext, cooccurents)){            \n",
    "                \n",
    "                let negWs = Vocab.samplingNegIndexs(posWs, 10);\n",
    "//                 console.log({w, posWs, negWs});\n",
    "                let uTensor = await memory.readSlots([w]);\n",
    "                let nuTensor = await memory.readSlots(posWs);\n",
    "                let nvTensor = await memory.readSlots(negWs);\n",
    "                \n",
    "                let l = await WordCoOccurentTraining(uTensor, nuTensor, nvTensor).data();\n",
    "                loss = [...loss, ...l];\n",
    "                await memory.writeSlots([w], uTensor);\n",
    "                await memory.writeSlots(posWs, nuTensor);\n",
    "                await memory.writeSlots(negWs, nvTensor);\n",
    "            }\n",
    "            console.log({epoch, lfile, loss: R.mean(loss)});\n",
    "        }\n",
    "        const CheckResult = async ()=>{\n",
    "            let targetWords = [0,1,2,3,4,5];\n",
    "            let topKTensor = await memory.getTopKSimilar(targetWords, 10);\n",
    "            topKTensor.print();\n",
    "            let topks = R.splitEvery(10, Array.from(await topKTensor.data()));\n",
    "            for(let [w, sim] of R.zip(targetWords, topks)){\n",
    "                let [tw, ...sws] = Vocab.indexToWord(sim);\n",
    "                termLogger.log(`[${tw}] is similar to ${sws.join(',')}`);\n",
    "            }    \n",
    "        }\n",
    "        await CheckResult();    \n",
    "    }\n",
    "    // tokenMatrix = CooccurenceMatrixReducer(tokenMatrix)(tokenContexts);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.writeFileSync('./cooccurent.matrix.json', JSON.stringify(tokenMatrix));\n",
    "fs.writeFileSync('./tokenMapper.json', JSON.stringify(mapper));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jp-Babel (Node.js)",
   "language": "babel",
   "name": "babel"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "11.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
